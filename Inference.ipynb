{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "from vqvae.vqvae import VQVAE\n",
    "from priors.transformer import VQNSynthTransformer\n",
    "\n",
    "from pytorch_nsynth import NSynth\n",
    "from GANsynth_pytorch.loader import (WavToSpectrogramDataLoader,\n",
    "                                     MaskedPhaseWavToSpectrogramDataLoader)\n",
    "from utils.misc import get_spectrograms_helper\n",
    "\n",
    "import GANsynth_pytorch\n",
    "from GANsynth_pytorch.utils import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "print(\"Initializing model\")\n",
    "vqvae_run_ID = 'VQVAE-20200914-120547-a8f38a'\n",
    "checkpoint_epoch = '19901'\n",
    "vqvae_folder = pathlib.Path(f'./data/checkpoints/{vqvae_run_ID}/')\n",
    "vqvae_weights_path = vqvae_folder / f'vqvae_nsynth_{checkpoint_epoch}.pt'\n",
    "vqvae_model_parameters_path = vqvae_folder/ 'model_parameters.json'\n",
    "vqvae_training_parameters_path = vqvae_folder / 'command_line_parameters.json'\n",
    "\n",
    "with open(vqvae_model_parameters_path, 'r') as f:\n",
    "    vqvae_model_parameters = json.load(f)\n",
    "with open(vqvae_training_parameters_path, 'r') as f:\n",
    "    vqvae_training_parameters = json.load(f)\n",
    "\n",
    "vqvae = VQVAE.from_parameters_and_weights(\n",
    "    vqvae_model_parameters_path,\n",
    "    vqvae_weights_path,\n",
    "    device=DEVICE\n",
    ")\n",
    "vqvae.to(DEVICE)\n",
    "vqvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pitch_range = [24, 84]\n",
    "\n",
    "dataset_audio_directory_paths = [\n",
    "        \"/home/theis/code/data/nsynth/train/audio\",\n",
    "        \"/home/theis/code/data/nsynth/valid/audio\"\n",
    "    ]\n",
    "validation_dataset_json_data_path = \"/home/theis/code/data/nsynth-balanced-split-fixed_seed/valid/examples.json\"\n",
    "    \n",
    "common_dataset_parameters = {\n",
    "        'valid_pitch_range': vqvae_training_parameters['valid_pitch_range'],\n",
    "        'categorical_field_list': [],\n",
    "        'squeeze_mono_channel': True\n",
    "    }\n",
    "nsynth_validation_dataset = NSynth(\n",
    "    audio_directory_paths=dataset_audio_directory_paths,\n",
    "    json_data_path=validation_dataset_json_data_path,\n",
    "    return_full_metadata=False,\n",
    "    **common_dataset_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vqvae_training_parameters['output_spectrogram_threshold']:\n",
    "    dataloader_class = MaskedPhaseWavToSpectrogramDataLoader\n",
    "else:\n",
    "    dataloader_class = WavToSpectrogramDataLoader\n",
    "\n",
    "spectrograms_helper = get_spectrograms_helper(DEVICE, **vqvae_training_parameters)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "validation_loader = dataloader_class(\n",
    "    dataset=nsynth_validation_dataset,\n",
    "    spectrograms_helper=spectrograms_helper,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=1, shuffle=True,\n",
    "    pin_memory=False)\n",
    "\n",
    "def make_audio(mag_and_IF_batch: torch.Tensor) -> np.ndarray:\n",
    "    audio_batch = spectrograms_helper.to_audio(mag_and_IF_batch)\n",
    "    audio_mono_concatenated = audio_batch.flatten().cpu().numpy()\n",
    "    return audio_mono_concatenated\n",
    "\n",
    "def make_audio_player(mag_and_IF_batch: torch.Tensor) -> None:\n",
    "    audio_mono_concatenated = make_audio(mag_and_IF_batch)\n",
    "    IPython.display.display(IPython.display.Audio(\n",
    "        audio_mono_concatenated,\n",
    "        rate=spectrograms_helper.fs_hz, normalize=True))\n",
    "    \n",
    "def plot_specs_and_IFs(*specs_and_IFs) -> None:    \n",
    "    num_subplots = 2*len(specs_and_IFs)\n",
    "    plots_per_row = 12\n",
    "    num_rows = num_subplots // plots_per_row + (\n",
    "        1 if num_subplots % plots_per_row != 0 or num_subplots < plots_per_row else 0)\n",
    "    plt.subplots(num_rows, plots_per_row, figsize=(25, 10*num_rows))\n",
    "    for subplot_index, spec_and_IF in enumerate(specs_and_IFs):\n",
    "        spec_and_IF = spec_and_IF.cpu().numpy()\n",
    "        ax_spec = plt.subplot(1, num_subplots, 1 + 2*subplot_index)\n",
    "        ax_IF = plt.subplot(1, num_subplots, 1+ 2*subplot_index + 1)\n",
    "        plots.plot_mel_representations(\n",
    "            spec_and_IF[0], spec_and_IF[1],\n",
    "            hop_length=spectrograms_helper.hop_length,\n",
    "            fs_hz=spectrograms_helper.fs_hz,\n",
    "            ax_spec=ax_spec, ax_IF=ax_IF)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def full_frame(width=None, height=None):\n",
    "    \"\"\"Initialize a full-frame matplotlib figure and axes\n",
    "\n",
    "    Taken from a GitHub Gist by Kile McDonald:\n",
    "    https://gist.github.com/kylemcdonald/bedcc053db0e7843ef95c531957cb90f\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams['savefig.pad_inches'] = 0\n",
    "    figsize = None if width is None else (width, height)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.axes([0, 0, 1, 1], frameon=False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.autoscale(tight=True)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "    '''\n",
    "    Heatmap with text in each cell with matplotlib's pyplot\n",
    "    Source: http://stackoverflow.com/a/25074150/395857 \n",
    "    By HYRY\n",
    "    '''\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.get_axes()\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    "def cm2inch(*tupl):\n",
    "    '''\n",
    "    Specify figure size in centimeter in matplotlib\n",
    "    Source: http://stackoverflow.com/a/22787457/395857\n",
    "    By gns-ank\n",
    "    '''\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "\n",
    "def heatmap(AUC, title=None, xlabel='Time steps', ylabel='Frequency', xticklabels=None, yticklabels=None, cmap='YlOrRd',\n",
    "           height_in=3, width_in=6, plot_colorbar=False):\n",
    "    '''\n",
    "    Inspired by:\n",
    "    - http://stackoverflow.com/a/16124677/395857 \n",
    "    - http://stackoverflow.com/a/25074150/395857\n",
    "    '''\n",
    "    AUC = AUC.detach().cpu().numpy()\n",
    "    # Plot it out\n",
    "#     fig, ax = plt.subplots()\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(width_in,height_in)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap, vmin=0.0, vmax=512.0)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    if xticklabels is not None:\n",
    "        ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "        ax.set_xticklabels(xticklabels, minor=False)\n",
    "    if yticklabels is not None:\n",
    "        ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "        ax.set_yticklabels(yticklabels, minor=False)\n",
    "    \n",
    "    if title is not None:\n",
    "        # set title and x/y labels\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)      \n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()\n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    if plot_colorbar:\n",
    "        # Add color bar\n",
    "        plt.colorbar(c)\n",
    "        \n",
    "    return fig, ax\n",
    "\n",
    "    # Add text in each cell \n",
    "#     show_values(c)\n",
    "\n",
    "    # resize \n",
    "#     fig = plt.gcf()\n",
    "#     fig.set_size_inches(cm2inch(40, 20))\n",
    "\n",
    "def make_spectrogram_image(spectrogram: torch.Tensor,\n",
    "                           filename: str = 'spectrogram',\n",
    "                           upsampling_factor: int = 1,\n",
    "                           ) -> pathlib.Path:\n",
    "    \"\"\"Generate and save a png image for the provided spectrogram.\n",
    "\n",
    "    Assumes melscale frequency axis.\n",
    "\n",
    "    Arguments:\n",
    "        spectrogram (torch.Tensor): the mel-scale spectrogram to draw\n",
    "    Returns:\n",
    "        output_path (str): the path where the image was written\n",
    "    \"\"\"\n",
    "    fig, ax = full_frame(width=12, height=8)\n",
    "    upsampled_spectrogram = (\n",
    "        torch.nn.functional.interpolate(\n",
    "            spectrogram.unsqueeze(0).unsqueeze(1),\n",
    "            mode='bilinear',\n",
    "            scale_factor=upsampling_factor)).squeeze(0).squeeze(0)\n",
    "    spectrogram_np = upsampled_spectrogram.cpu().numpy()\n",
    "    librosa.display.specshow(spectrogram_np,\n",
    "                             #  y_axis='mel',\n",
    "                             ax=ax,\n",
    "                             sr=spectrograms_helper.fs_hz * upsampling_factor,\n",
    "                             cmap='viridis',\n",
    "                             hop_length=spectrograms_helper.hop_length)\n",
    "\n",
    "    image_format = 'pdf'\n",
    "    # output_path = tempfile.mktemp() + '.' + image_format\n",
    "    output_path = filename + '.' + image_format\n",
    "    fig.savefig(output_path, format=image_format, dpi=200,\n",
    "                pad_inches=0, bbox_inches=0)\n",
    "    fig.clear()\n",
    "    plt.close()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_reconstructions(loader=validation_loader):\n",
    "    vqvae.eval()\n",
    "    samples, *_ = next(iter(loader))\n",
    "    samples.to(DEVICE)\n",
    "    reconstructions, *_, id_top, id_bottom = vqvae.forward(samples)\n",
    "    return samples, reconstructions, id_top, id_bottom\n",
    "\n",
    "samples, reconstructions, *_ = sample_reconstructions()\n",
    "\n",
    "def print_stats(melspec_and_IF_batch: torch.Tensor) -> None:\n",
    "    logmelspec_batch = melspec_and_IF_batch[:,0]\n",
    "    melIF_batch = melspec_and_IF_batch[:,1]\n",
    "    print(f'Log-mel-spec: Mean {logmelspec_batch.mean().item():.2f}, variance {logmelspec_batch.var().item():.2f}')\n",
    "    print(f'Mel-IF: Mean {melIF_batch.mean().item():.2f}, variance {melIF_batch.var().item():.2f}')\n",
    "\n",
    "print(\"Original audio\")\n",
    "print_stats(samples)\n",
    "make_audio_player(samples)\n",
    "\n",
    "print(\"Reconstructed audio\")\n",
    "print_stats(reconstructions)\n",
    "# using shortcut helper function in the rest of this notebook\n",
    "make_audio_player(reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_loader = dataloader_class(\n",
    "    dataset=nsynth_validation_dataset,\n",
    "    spectrograms_helper=spectrograms_helper,\n",
    "    batch_size=64,\n",
    "    num_workers=10, shuffle=True,\n",
    "    pin_memory=False)\n",
    "large_batch, *_ = sample_reconstructions(large_loader)\n",
    "print((large_batch - vqvae.data_normalizer.denormalize(vqvae.data_normalizer.normalize(large_batch))).norm()/large_batch.norm())\n",
    "print((vqvae.data_normalizer.normalize(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_vqvae import get_reconstruction_criterion\n",
    "\n",
    "jukebox_criterion = get_reconstruction_criterion('Jukebox',\n",
    "                                                spectrograms_helper)\n",
    "\n",
    "samples, reconstructions, *_ = sample_reconstructions()\n",
    "for set_name, t in {'Samples': samples, 'Reconstructions': reconstructions}.items():\n",
    "    print(set_name)\n",
    "    print(\"Mean: \", torch.mean(jukebox_criterion.squared_magnitude(t)))\n",
    "    print(\"Variance: \", torch.var(jukebox_criterion.squared_magnitude(t)))\n",
    "    print(\"Min: \", torch.min(jukebox_criterion.squared_magnitude(t)))\n",
    "    print(\"Max: \", torch.max(jukebox_criterion.squared_magnitude(t)))\n",
    "    print('\\n======\\n')\n",
    "\n",
    "print(\"Jukebox loss: \", jukebox_criterion(samples, reconstructions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stfts():\n",
    "    samples, reconstructions, *_ = sample_reconstructions()\n",
    "    samples_audio = spectrograms_helper.to_audio(samples)\n",
    "    samples_reconstructions = spectrograms_helper.to_audio(reconstructions)\n",
    "\n",
    "    spec_mag_samples_list = []\n",
    "    spec_mag_reconstructions_list = []\n",
    "    for (n_fft, window_length, window) in zip(jukebox_criterion.n_ffts,\n",
    "                                              jukebox_criterion.window_lengths,\n",
    "                                              jukebox_criterion.windows):\n",
    "        print(n_fft, window_length)\n",
    "        hop_length = math.ceil((1-jukebox_criterion.overlap_ratio) * window_length)\n",
    "\n",
    "        spec_mag_samples, spec_mag_reconstructions = (\n",
    "            jukebox_criterion.magnitude(torch.stft(\n",
    "                    audio, n_fft=n_fft, hop_length=hop_length,\n",
    "                    win_length=window_length,\n",
    "                    window=window, center=False))\n",
    "                for audio in (samples_audio, samples_reconstructions)\n",
    "            )\n",
    "        spec_mag_samples_list.append(spec_mag_samples)\n",
    "        spec_mag_reconstructions_list.append(spec_mag_reconstructions)\n",
    "    return spec_mag_samples_list, spec_mag_reconstructions_list\n",
    "spec_mag_samples_list, spec_mag_reconstructions_list = get_stfts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_index = 2\n",
    "nn.L1Loss()(spec_mag_samples_list[scale_index], spec_mag_reconstructions_list[scale_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump audio samples of reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./paper/reconstructions.json', 'r') as f:\n",
    "    reconstructions_details = json.load(f)\n",
    "\n",
    "reconstructions_note_strs = list(reconstructions_details.keys())\n",
    "lookup_table = {nsynth_validation_dataset[i][1]['note_str']: i\n",
    "                for i in range(len(nsynth_validation_dataset))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions_samples = [\n",
    "    nsynth_validation_dataset[lookup_table[note_str]]\n",
    "    for note_str in reconstructions_note_strs]\n",
    "reconstructions_loader = dataloader_class(\n",
    "    dataset=reconstructions_samples,\n",
    "    spectrograms_helper=spectrograms_helper,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=8, shuffle=True,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import json\n",
    "\n",
    "samples_dir = pathlib.Path('./paper/reconstructions')\n",
    "\n",
    "samples, reconstructions, id_top, id_bottom, annotations = sample_reconstructions(reconstructions_loader)\n",
    "NORMALIZE_AUDIO = True\n",
    "reconstructions_details = {}\n",
    "for sample, reconstruction, note_str, pitch, instrument_family_str in zip(\n",
    "        samples, reconstructions,\n",
    "        annotations['note_str'], annotations['pitch'], annotations['instrument_family_str']):\n",
    "    for audio_format in ['wav']:\n",
    "        filename_original = samples_dir / 'audio' / f'{note_str}-original.{audio_format}' \n",
    "        sample_audio = make_audio(sample.unsqueeze(0))\n",
    "        if NORMALIZE_AUDIO:\n",
    "            sample_audio /= abs(sample_audio.max())\n",
    "        \n",
    "        sf.write(filename_original, sample_audio,\n",
    "                 samplerate=spectrograms_helper.fs_hz, format=audio_format)\n",
    "\n",
    "        filename_reconstruction = samples_dir / 'audio' / f'{note_str}-reconstruction.{audio_format}'\n",
    "        reconstruction_audio = make_audio(reconstruction.unsqueeze(0))\n",
    "        if NORMALIZE_AUDIO:\n",
    "            reconstruction_audio /= abs(reconstruction_audio.max())\n",
    "        sf.write(str(filename_reconstruction), reconstruction_audio,\n",
    "                 samplerate=spectrograms_helper.fs_hz, format=audio_format)\n",
    "    reconstructions_details[note_str] = {'pitch': int(pitch.data), 'instrument_family_str': instrument_family_str}\n",
    "\n",
    "with open(samples_dir / 'reconstructions.json', 'w') as f:\n",
    "    json.dump(reconstructions_details, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['note_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spectrogram_image_to_file(logmel_and_IF, filename_base, hop_length, fs_hz,\n",
    "                                    width=None, height=None):\n",
    "    for channel_index, channel_name in enumerate(['logmel', 'IF']):\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        fig, ax = full_frame(width=width, height=height)\n",
    "        librosa.display.specshow(logmel_and_IF[channel_index].cpu().numpy(),\n",
    "                                 sr=fs_hz, hop_length=hop_length,\n",
    "                                 ax=ax)\n",
    "        plt.savefig(str(filename_base.resolve()) + f'-{channel_name}.png')\n",
    "\n",
    "WIDTH = 6\n",
    "HEIGHT = 4\n",
    "images_samples_dir = samples_dir / 'images'\n",
    "images_samples_dir.mkdir(exist_ok=True)\n",
    "for sample, reconstruction, note_str in zip(samples, reconstructions,\n",
    "                                            annotations['note_str']):\n",
    "    for audio, audio_name in zip([sample, reconstruction],\n",
    "                                 ['original', 'reconstruction']):\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        filename_image_base = images_samples_dir / f'{note_str}-{audio_name}'\n",
    "        write_spectrogram_image_to_file(audio, filename_image_base,\n",
    "                                        spectrograms_helper.hop_length,\n",
    "                                        spectrograms_helper.fs_hz,\n",
    "                                        width=WIDTH, height=HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -C paper -czf paper/reconstructions.tar.gz reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 11000\n",
    "single_sample_loader = dataloader_class(\n",
    "    dataset=[nsynth_validation_dataset[sample_index]],\n",
    "    spectrograms_helper=spectrograms_helper,\n",
    "    batch_size=1,\n",
    "    num_workers=8, shuffle=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "sample_note_str = nsynth_validation_dataset[sample_index][1]['note_str']\n",
    "acoustic_guitar_sample, acoustic_guitar_reconstruction, acoustic_guitar_id_top, acoustic_guitar_id_bottom, annotations = sample_reconstructions(single_sample_loader)\n",
    "spectrograms_comparison_path = pathlib.Path('./paper/spectrograms_comparison/')\n",
    "spectrograms_comparison_path.mkdir(exist_ok=True)\n",
    "\n",
    "filename_reconstruction = spectrograms_comparison_path / 'audio' / f\"{sample_note_str}-{vqvae_run_ID}_{checkpoint_epoch}-reconstruction.wav\"\n",
    "filename_reconstruction.parent.mkdir(parents=True, exist_ok=True)\n",
    "acoustic_guitar_reconstruction_audio = make_audio(acoustic_guitar_reconstruction)\n",
    "if NORMALIZE_AUDIO:\n",
    "    acoustic_guitar_reconstruction_audio /= abs(acoustic_guitar_reconstruction_audio.max())\n",
    "sf.write(str(filename_reconstruction), acoustic_guitar_reconstruction_audio,\n",
    "         samplerate=spectrograms_helper.fs_hz, format=audio_format)\n",
    "\n",
    "(spectrograms_comparison_path / 'images/').mkdir(exist_ok=True, parents=True)\n",
    "base_filename = str((spectrograms_comparison_path / 'images' / f\"{sample_note_str}-{vqvae_run_ID}_{checkpoint_epoch}\").resolve())\n",
    "\n",
    "write_spectrogram_image_to_file(\n",
    "    acoustic_guitar_sample[0].detach(),\n",
    "    pathlib.Path(base_filename + '-original'),\n",
    "    spectrograms_helper.hop_length, spectrograms_helper.fs_hz)\n",
    "write_spectrogram_image_to_file(\n",
    "    acoustic_guitar_reconstruction[0].detach(),\n",
    "    pathlib.Path(base_filename+ '-reconstruction'),\n",
    "    spectrograms_helper.hop_length, spectrograms_helper.fs_hz)\n",
    "\n",
    "heatmap(acoustic_guitar_id_top[0], cmap='magma')\n",
    "plt.savefig(base_filename+ '-top.png')\n",
    "heatmap(acoustic_guitar_id_bottom[0], cmap='magma')\n",
    "plt.savefig(base_filename+ '-bottom.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -C paper -cvf spectrograms_comparison.tar.gz spectrograms_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction  without using all hidden layers\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples, _ = next(iter(single_sample_loader))\n",
    "    quant_t, quant_b, *_ = vqvae.encode(samples.to(device))\n",
    "    decoded_only_bottom = vqvae.decode(torch.zeros(quant_t.shape).to(device), quant_b).cpu()\n",
    "    decoded_only_top = vqvae.decode(quant_t, torch.zeros(quant_b.shape).to(device)).cpu()\n",
    "    decoded = vqvae.decode(quant_t, quant_b).cpu()\n",
    "\n",
    "specs_and_IFs = [samples[0], decoded_only_bottom[0], decoded_only_top[0], decoded[0]]\n",
    "plot_specs_and_IFs(specs_and_IFs)\n",
    "# plt.suptitle('Original, Reconstructed using only bottom / only top / top and bottom', pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display computed codes\n",
    "named_code_layers = {'top': vqvae.quantize_t, 'bottom': vqvae.quantize_b}\n",
    "num_layers = len(named_code_layers.values())\n",
    "fig, _ = plt.subplots(num_layers, 2, figsize=(20, 5*num_layers))\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "for layer_index, (layer_name, code_layer) in enumerate(named_code_layers.items()):\n",
    "    codes = code_layer.embed.data.cpu().T\n",
    "    ax = plt.subplot(num_layers, 2, 1 + 2*layer_index)\n",
    "    codes_matrix = codes - codes.mean(dim=0, keepdim=True)\n",
    "    plt.matshow(codes_matrix, fignum=0)\n",
    "    plt.title(f'{layer_name} layer codes (layer number {layer_index+1})', pad=20)\n",
    "    ax = plt.subplot(num_layers, 2, 1 + 2*layer_index + 1)\n",
    "#     codes_selfsimilarity = codes_matrix @ codes_matrix.T\n",
    "    codes_correlation = np.corrcoef(codes_matrix)\n",
    "    pairwise_correlations = pdist(codes_matrix + 1e-7, 'correlation')\n",
    "    \n",
    "#     print(pairwise_cosine_similarity)\n",
    "#     plt.matshow(1 - abs(codes_correlation), fignum=0)\n",
    "    plt.hist(pairwise_correlations, bins=30)\n",
    "#     plt.colorbar()\n",
    "    plt.title(f'{str(layer_name).capitalize()} layer codes absolute correlation \\n (layer number {layer_index+1})', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code distribution over the training dataset\n",
    "num_embeddings_top = vqvae.quantize_t.n_embed\n",
    "num_embeddings_bottom = vqvae.quantize_b.n_embed\n",
    "code_usage_top = torch.zeros(num_embeddings_top).long().to(device)\n",
    "code_usage_bottom = torch.zeros(num_embeddings_bottom).long().to(device)\n",
    "\n",
    "def count_uses(codes_batch, code_usage_matrix):\n",
    "    num_codes = len(code_usage_matrix)\n",
    "    code_usage_batch = codes_batch.flatten().bincount(minlength=num_codes)\n",
    "    code_usage_matrix += code_usage_batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, pitch in tqdm(loader):\n",
    "        _, _, _, id_t, id_b, _, _ = vqvae.encode(batch.to(device))\n",
    "        count_uses(id_t, code_usage_top)\n",
    "        count_uses(id_b, code_usage_bottom)\n",
    "\n",
    "plt.subplots(2, 1, figsize=(30, 15))\n",
    "plt.subplot(2, 1, 1, title=\"Bottom layer\")\n",
    "plt.bar(range(num_embeddings_bottom), height=list(code_usage_bottom.cpu().numpy()))\n",
    "plt.subplot(2, 1, 2, title=\"Top layer\")\n",
    "plt.bar(range(num_embeddings_top), height=list(code_usage_top.cpu().numpy()))\n",
    "plt.savefig(f'code_usage-epoch_{checkpoint_epoch}.svg')\n",
    "\n",
    "plt.suptitle(\"Code usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings dimension comparison\n",
    "\n",
    "In this section, we compare the dimension of the training/target data and of the embeddings,\n",
    "in order to compute and check if the compression ratio actually makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_single_sample_iterator = iter(siptngle_sample_loader)\n",
    "sample_as_batch, _ = next(one_shot_single_sample_iterator)\n",
    "sample_as_batch\n",
    "sample = sample_as_batch[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    *results_as_batches, _, _ = vqvae.encode(sample_as_batch.to(device))\n",
    "    quant_t, quant_b, diff_t_plus_b, id_t, id_b = [\n",
    "        tensor[0].cpu() for tensor in results_as_batches]\n",
    "\n",
    "print('Original shape: ', sample.shape)\n",
    "print('Quant bottom shape: ', quant_b.shape)\n",
    "print('Quant top shape: ', quant_t.shape)\n",
    "print('Code bottom shape: ', id_b.shape)\n",
    "print('Code top shape: ', id_t.shape)\n",
    "print(\"Compression ratio: \", (id_b.numel() + id_t.numel()) / sample.numel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound creation\n",
    "\n",
    "In this section, we attempt to create new sounds by combining codes for heterogeneous sounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sample_loader = WavToSpectrogramDataLoader(nsynth_dataset, batch_size=2,\n",
    "                                               num_workers=2, shuffle=True)\n",
    "one_shot_two_sample_iterator = iter(two_sample_loader)\n",
    "with torch.no_grad():\n",
    "    samples, _ = next(one_shot_two_sample_iterator)\n",
    "    quant_t, quant_b, _, id_t, id_b, _, _ = vqvae.encode(samples.to(device))\n",
    "\n",
    "print(\"Original audios\")\n",
    "make_audio_player(samples)\n",
    "\n",
    "print('Reconstructions')\n",
    "with torch.no_grad():\n",
    "    reconstructions = vqvae.decode_code(id_t, id_b)\n",
    "make_audio_player(reconstructions)\n",
    "\n",
    "print('Exchange top and bottom codes')\n",
    "with torch.no_grad():\n",
    "    reconstructions_switched_codes = vqvae.decode_code(id_t, id_b.flip(0))\n",
    "make_audio_player(reconstructions_switched_codes)\n",
    "\n",
    "plot_specs_and_IFs(*samples[0:2], *reconstructions[0:2], *reconstructions_switched_codes[0:2])\n",
    "\n",
    "# print('Exchange half of the codes (in the temporal dimension)')\n",
    "# def switch_temporal_halves(codes_tensor: torch.Tensor) -> torch.Tensor:\n",
    "#     batch_dim, frequency_dim, temporal_dim = 0, 1, 2\n",
    "#     duration_n = codes_tensor.shape[temporal_dim]\n",
    "#     first_half = codes_tensor[:, :, :duration_n//2]\n",
    "#     second_half = codes_tensor[:, :, duration_n//2:]\n",
    "#     return torch.cat([first_half.flip(0), second_half], temporal_dim)\n",
    "# with torch.no_grad():\n",
    "#     reconstructions_halves_switched = vqvae.decode_code(\n",
    "#         *[switch_temporal_halves(codes) for codes in [id_t, id_b]])\n",
    "# make_audio_player(reconstructions_halves_switched)\n",
    "\n",
    "# print('Set half (temporally) of the latent map to zero')\n",
    "# def zero_temporal_halves(codes_tensor: torch.Tensor) -> torch.Tensor:\n",
    "#     batch_dim, frequency_dim, temporal_dim = 0, 1, 2\n",
    "#     duration_n = codes_tensor.shape[temporal_dim]\n",
    "#     first_half = codes_tensor[:, :, :duration_n//2]\n",
    "#     zero_second_half = torch.zeros(codes_tensor[:, :, duration_n//2:].shape, dtype=first_half.dtype).to(device)\n",
    "#     return torch.cat([first_half, zero_second_half], dim=temporal_dim)\n",
    "# with torch.no_grad():\n",
    "#     reconstructions_halves_zeroed = vqvae.decode_code(\n",
    "#         *[zero_temporal_halves(codes) for codes in [id_t, id_b]])\n",
    "# make_audio_player(reconstructions_halves_zeroed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear interpolation of the embedddings')\n",
    "def make_interpolations(start_map, end_map, steps):\n",
    "    ratios = torch.linspace(0, 1, steps).to(device)\n",
    "    translations = torch.einsum('i, jkl -> ijkl', ratios, (end_map - start_map))\n",
    "    return start_map + translations\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_steps = 5\n",
    "    interpolations_unquant_t_and_b = [\n",
    "        make_interpolations(codes[0], codes[1], num_steps)\n",
    "        for codes in (quant_t, quant_b)]\n",
    "    print(interpolations_unquant_t_and_b[0][:,0,0])\n",
    "    print(interpolations_unquant_t_and_b[0].shape)\n",
    "    print(interpolations_unquant_t_and_b[1].shape)\n",
    "    \n",
    "    interpolations_quant_t, interpolations_quant_b = [\n",
    "        quantizer(interpolations_unquant.permute(0, 2, 3, 1))[0].permute(0, 3, 1, 2)\n",
    "        for quantizer, interpolations_unquant in zip(\n",
    "            [vqvae.quantize_t, vqvae.quantize_b],\n",
    "            interpolations_unquant_t_and_b)\n",
    "    ]\n",
    "    \n",
    "    print(interpolations_quant_t[:,0,0])\n",
    "    interpolations = vqvae.decode(interpolations_quant_t, interpolations_quant_b)\n",
    "for index, interpolation in enumerate(interpolations):\n",
    "    print(f'Step {index}')\n",
    "    make_audio_player(interpolation)\n",
    "\n",
    "# comparing with the sum of the two signals in temporal domain\n",
    "make_audio_player(reconstructions.sum(0)/2)\n",
    "specs_and_IFs = [samples[0], samples[1], interpolations[1], interpolations[2], interpolations[3], samples.sum(0)]\n",
    "plot_specs_and_IFs(*specs_and_IFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: don't feed latent maps to quantizer by reshaping them, use .permute()\n",
    "# Otherwise, the results are scrambled!\n",
    "with torch.no_grad():\n",
    "    embeddings_t = quant_t[0].reshape(128, 16, 64)\n",
    "    requantized_embeddings_t = vqvae.quantize_t(embeddings_t)[0]\n",
    "    display(requantized_embeddings_t - embeddings_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent-codes distortion\n",
    "\n",
    "Here, we slightly modify the latent maps to check the effect on the decoded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, reconstruction = inference_vqvae.sample_reconstructions(single_sample_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    *results_as_batches, _, _ = vqvae.encode(sample.to(device))\n",
    "    quant_t, quant_b, diff_t_plus_b, id_t, id_b = [\n",
    "        tensor.cpu() for tensor in results_as_batches]\n",
    "\n",
    "all_code_layers = {'top': id_t, 'bottom': id_b}\n",
    "code_layers_to_corrupt_names = ['bottom']\n",
    "code_layers_to_keep_names = list(set(all_code_layers.keys()) - set(code_layers_to_corrupt_names))\n",
    "num_embeddings = {'top': vqvae.quantize_t.n_embed,\n",
    "                  'bottom': vqvae.quantize_b.n_embed}\n",
    "\n",
    "add_values = [+1, -1, +10, -10, +20, -20, +100, -100, +200, -200]\n",
    "id_plus_corrupted = {\n",
    "    code_layer_name: torch.cat([all_code_layers[code_layer_name]\n",
    "                                .add(add_value)\n",
    "                                .remainder(num_embeddings[code_layer_name])\n",
    "                                .long()\n",
    "     for add_value in add_values\n",
    "    ], 0)\n",
    "    for code_layer_name in code_layers_to_corrupt_names\n",
    "}\n",
    "\n",
    "id_plus_original = {\n",
    "    code_layer_name: all_code_layers[code_layer_name].repeat(len(add_values), 1, 1)\n",
    "    for code_layer_name in code_layers_to_keep_names\n",
    "}\n",
    "\n",
    "def pick_codes(original, corrupted):\n",
    "    codes = [(original.get(code_layer_name, None) if original.get(code_layer_name, None) is not None\n",
    "             else corrupted[code_layer_name]).to(device)\n",
    "            for code_layer_name in all_code_layers.keys()]\n",
    "    return codes\n",
    "\n",
    "# weights = torch.Tensor([0.2, 0.6, 0.2])\n",
    "# id_stochastic_1 = [\n",
    "#     id_map.add(torch.multinomial(weights, id_map.numel(), replacement=True).reshape(id_map.shape).add(-1).to(device)).remainder(n_embed).long()\n",
    "#     for id_map, n_embed in zip([id_t, id_b],\n",
    "#                                [vqvae.quantize_t.n_embed, vqvae.quantize_b.n_embed])\n",
    "# ]\n",
    "\n",
    "# id_stochastic_randn_0_20_corrupted = [\n",
    "#     id_map.add((torch.randn_like(id_map, dtype=float).to(device) * 20).long()).remainder(n_embed).long()\n",
    "#     for id_map, n_embed in zip(codes_to_corrupt,\n",
    "#                                num_embeddings_to_corrupt)\n",
    "# ]\n",
    "\n",
    "# print(id_stochastic_randn_0_20_corrupted)\n",
    "# print(codes_to_keep)\n",
    "# id_stochastic = id_stochastic_randn_0_20_corrupted + codes_to_keep\n",
    "# print(id_stochastic)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructions_plus = vqvae.decode_code(*pick_codes(id_plus_corrupted, id_plus_original))\n",
    "#     reconstructions_stochastic_1 = vqvae.decode_code(*id_stochastic_1)\n",
    "#     reconstructions_stochastic_randn_0_20 = vqvae.decode_code(*id_stochastic)\n",
    "\n",
    "print('Original')\n",
    "make_audio_player(sample)\n",
    "\n",
    "print('Reconstructions')\n",
    "make_audio_player(reconstruction)\n",
    "    \n",
    "for index, add_value in enumerate(add_values):\n",
    "    print('Uniform add', add_value)\n",
    "    make_audio_player(reconstructions_plus[index])\n",
    "\n",
    "# make_audio_player(reconstructions_stochastic_1)\n",
    "\n",
    "# make_audio_player(reconstructions_stochastic_randn_0_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.Tensor([0.2, 0.6, 0.2])\n",
    "id_stochastic = [\n",
    "    torch.multinomial(weights, id_map.numel(), replacement=True).reshape(id_map.shape).to(device).long()\n",
    "    for id_map, n_embed in zip([id_t, id_b],\n",
    "                               [vqvae.quantize_t.n_embed, vqvae.quantize_b.n_embed])\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructions_stochastic = vqvae.decode_code(*id_stochastic)\n",
    "make_audio_player(reconstructions_stochastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing codemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample, reconstructions, id_top, id_bottom = sample_reconstructions()\n",
    "import librosa.display\n",
    "\n",
    "make_audio_player(sample[:1])\n",
    "plt.clf()\n",
    "plt.close()\n",
    "# plots.plot_mel_representations(*[sample[0, i].cpu().numpy() for i in (0, 1)],\n",
    "#                                hop_length=spectrograms_helper.hop_length, fs_hz=spectrograms_helper.fs_hz,\n",
    "#                                 print_title=False)\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "librosa.display.specshow(sample[0, 0].cpu().numpy(), sr=spectrograms_helper.fs_hz,\n",
    "                         hop_length=spectrograms_helper.hop_length, ax=ax)\n",
    "plt.savefig('mel_spec_amplitude.pdf')\n",
    "\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax = librosa.display.specshow(sample[0, 1].cpu().numpy(), sr=spectrograms_helper.fs_hz,\n",
    "                              hop_length=spectrograms_helper.hop_length, ax=ax)\n",
    "plt.savefig('mel_IF.pdf')\n",
    "\n",
    "make_spectrogram_image(sample[0,0], filename='upsampled_spectrogram',\n",
    "                       upsampling_factor=4)\n",
    "\n",
    "\n",
    "make_spectrogram_image(reconstructions[0,0], filename='upsampled_reconstructed_spectrogram',\n",
    "                       upsampling_factor=4)\n",
    "\n",
    "\n",
    "make_spectrogram_image(sample[0,0].flip(0), filename='flipped-upsampled_spectrogram',\n",
    "                       upsampling_factor=4)\n",
    "make_spectrogram_image(reconstructions[0,0].flip(0), filename='flipped-upsampled_reconstructed_spectrogram',\n",
    "                       upsampling_factor=4)\n",
    "\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "heatmap(id_top[0], \"Top\", cmap='magma')\n",
    "heatmap(id_bottom[0], \"Bottom\", cmap='magma')\n",
    "\n",
    "flattened_top = transformer_top.flatten_map(id_top[:1], kind='source')\n",
    "heatmap(flattened_top, cmap='magma', height_in=1.5)\n",
    "flattened_bottom = transformer_bottom.flatten_map(id_bottom[:1], kind='target')\n",
    "heatmap(flattened_bottom, cmap='magma', height_in=1.5)\n",
    "\n",
    "\n",
    "heatmap(id_top[0,:8,:1], cmap='magma')\n",
    "plt.savefig('top_extract_1s_first_quarter_of_frequency_axis.pdf')\n",
    "heatmap(id_bottom[0,:16,:2], cmap='magma')\n",
    "plt.savefig('bottom_extract_1s_first_quarter_of_frequency_axis.pdf')\n",
    "\n",
    "flattened_top = transformer_top.flatten_map(id_top[:1], kind='source')\n",
    "print(\"Flattened top, sample\")\n",
    "heatmap(flattened_top[:,:8], cmap='magma', height_in=1.5)\n",
    "plt.savefig('top_flattened_extract_1s_first_quarter_of_frequency_axis.pdf')\n",
    "print(\"Flattened bottom, sample\")\n",
    "flattened_bottom = transformer_bottom.flatten_map(id_bottom[:1], kind='target')\n",
    "heatmap(flattened_bottom[:,:32], cmap='magma', height_in=1.5)\n",
    "plt.savefig('bottom_flattened_extract_1s_first_quarter_of_frequency_axis.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_top_edited_fragment = flattened_top[:,:8].clone()\n",
    "flattened_top_edited_fragment[:,-3] = (flattened_top_edited_fragment[:,-3] + 666) % 512 \n",
    "heatmap(flattened_top[:,:8], cmap='magma', height_in=1.5)\n",
    "heatmap(flattened_top_edited_fragment, cmap='magma', height_in=1.5)\n",
    "plt.savefig('edited-top_flattened_extract_1s_first_quarter_of_frequency_axis.pdf')\n",
    "print(\"Flattened bottom, sample\")\n",
    "flattened_bottom_edited_fragment = flattened_bottom[:,:32].clone()\n",
    "flattened_bottom_edited_fragment[:,-4*3:-4*2] = ((4 + flattened_bottom_edited_fragment[:,-4*3:-4*2]) ** 3) % 512\n",
    "heatmap(flattened_bottom[:,:32], cmap='magma', height_in=1.5)\n",
    "heatmap(flattened_bottom_edited_fragment, cmap='magma', height_in=1.5)\n",
    "plt.savefig('edited-bottom_flattened_extract_1s_first_quarter_of_frequency_axis.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_top_edited = flattened_top.clone()\n",
    "flattened_top_edited[:,:8] = flattened_top_edited_fragment\n",
    "flattened_bottom_edited = flattened_bottom.clone()\n",
    "flattened_bottom_edited[:,:32] = flattened_bottom_edited_fragment\n",
    "\n",
    "id_top_edited = transformer_top.to_time_frequency_map(flattened_top_edited, kind='target')\n",
    "id_bottom_edited = transformer_bottom.to_time_frequency_map(flattened_bottom_edited, kind='target')\n",
    "\n",
    "heatmap(id_top_edited[0,:8,:1], cmap='magma')\n",
    "plt.savefig('edited-top_extract_1s_first_quarter_of_frequency_axis.pdf')\n",
    "heatmap(id_bottom_edited[0,:16,:2], cmap='magma')\n",
    "plt.savefig('edited-bottom_extract_1s_first_quarter_of_frequency_axis.pdf')\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = vqvae.decode_code(id_top_edited, id_bottom_edited)\n",
    "\n",
    "make_spectrogram_image(reconstructed[0,0], filename='upsampled_edited_spectrogram',\n",
    "                       upsampling_factor=4)\n",
    "make_spectrogram_image(reconstructed[0,0].flip(0), filename='flipped-upsampled_edited_spectrogram',\n",
    "                       upsampling_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sample import time_stretch_and_resample\n",
    "\n",
    "time_stretch_and_resample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_root_path = pathlib.Path(f'./checkpoints/code_prediction/vqvae-{vqvae_run_ID}/')\n",
    "\n",
    "transformer_top_run_id = 'Transformer-top_layer-20200513-231538-0bd9f5' \n",
    "transformer_top_folder = transformers_root_path / transformer_top_run_id\n",
    "transformer_top_weights_path = transformer_top_folder / 'Transformer-layer_top.pt'\n",
    "transformer_top_model_parameters_path = transformer_top_folder / 'model_instantiation_parameters.json'\n",
    "\n",
    "transformer_bottom_run_id = 'Transformer-bottom_layer-20200512-165540-9964e5' \n",
    "transformer_bottom_folder = transformers_root_path / transformer_bottom_run_id\n",
    "transformer_bottom_weights_path = transformer_bottom_folder / 'Transformer-layer_bottom.pt'\n",
    "transformer_bottom_model_parameters_path = transformer_bottom_folder / 'model_instantiation_parameters.json'\n",
    "\n",
    "transformer_top = VQNSynthTransformer.from_parameters_and_weights(\n",
    "    transformer_top_model_parameters_path,\n",
    "    transformer_top_weights_path,\n",
    "    device=DEVICE\n",
    ")\n",
    "transformer_top = transformer_top.eval()\n",
    "\n",
    "transformer_bottom = VQNSynthTransformer.from_parameters_and_weights(\n",
    "    transformer_bottom_model_parameters_path,\n",
    "    transformer_bottom_weights_path,\n",
    "    device=DEVICE\n",
    ")\n",
    "transformer_bottom = transformer_bottom.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sample import sample_model, make_conditioning_tensors\n",
    "from dataset import LMDBDataset\n",
    "import soundfile as sf\n",
    "\n",
    "classes_for_conditioning = ['pitch', 'instrument_family_str']\n",
    "DATABASE_PATH = pathlib.Path('./codes/vqvae-20200309-220303-d006ab-weights-vqvae_nsynth_436/train/').resolve()\n",
    "dataset = LMDBDataset(\n",
    "    DATABASE_PATH,\n",
    "    classes_for_conditioning=list(classes_for_conditioning)\n",
    ")\n",
    "label_encoders_per_conditioning = dataset.label_encoders\n",
    "\n",
    "all_instrument_labels = (\n",
    "    label_encoders_per_conditioning['instrument_family_str']\n",
    "    .classes_\n",
    "    .tolist())\n",
    "\n",
    "NORMALIZE_AUDIO = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconditioned sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconditional_sampling_pitches = np.arange(24, 85, 7)\n",
    "unconditional_sampling_instruments = all_instrument_labels\n",
    "num_pitches = len(unconditional_sampling_pitches)\n",
    "num_instruments = len(unconditional_sampling_instruments)\n",
    "num_samples = num_pitches * num_instruments\n",
    "print(f\"Will generate a total of {num_samples} samples, \"\n",
    "      f\"from {num_instruments} different instruments\")\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "class_conditioning_tensors = {}\n",
    "\n",
    "encoded_pitches = label_encoders_per_conditioning['pitch'].transform(\n",
    "    unconditional_sampling_pitches).transpose()\n",
    "class_conditioning_tensors['pitch'] = (\n",
    "    torch.from_numpy(encoded_pitches).long()\n",
    "    .to(DEVICE))\n",
    "\n",
    "unconditional_generation_path = pathlib.Path(f'./paper/unconditional_generation/{vqvae_run_ID}_{checkpoint_epoch}')\n",
    "unconditional_generation_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for instrument in ['bass']:\n",
    "    encoded_instrument = label_encoders_per_conditioning['instrument_family_str'].transform(\n",
    "        [instrument]).transpose()\n",
    "    class_conditioning_tensors['instrument_family_str'] = (\n",
    "        torch.from_numpy(encoded_instrument).long().repeat(num_pitches)\n",
    "        .to(DEVICE))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        transformer_top.to(DEVICE)\n",
    "        transformer_top.eval()\n",
    "        sampled_top = sample_model(\n",
    "            transformer_top, device=DEVICE, batch_size=num_pitches,\n",
    "            class_conditioning=class_conditioning_tensors,\n",
    "            codemap_size=transformer_top.shape,\n",
    "            temperature=1.0, top_p_sampling_p=0.8,\n",
    "            use_multi_gpus=False)\n",
    "        \n",
    "        transformer_bottom.to(DEVICE)\n",
    "        transformer_bottom.eval()\n",
    "        sampled_bottom = sample_model(\n",
    "            transformer_bottom, device=DEVICE, batch_size=num_pitches,\n",
    "            condition=sampled_top,\n",
    "            class_conditioning=class_conditioning_tensors,\n",
    "            codemap_size=transformer_bottom.shape,\n",
    "            temperature=1.0, top_p_sampling_p=0.8,\n",
    "            use_multi_gpus=False)\n",
    "\n",
    "        vqvae.to(DEVICE)\n",
    "        vqvae.eval()\n",
    "        samples_batch = vqvae.decode_code(sampled_top, sampled_bottom)\n",
    "        for pitch, sample in zip(unconditional_sampling_pitches, samples_batch):\n",
    "            filename = unconditional_generation_path / f'{instrument}-{pitch}.wav'\n",
    "            audio = spectrograms_helper.to_audio(sample.unsqueeze(0)).cpu().numpy()\n",
    "            if NORMALIZE_AUDIO:\n",
    "                sample /= abs(sample.max())\n",
    "            sf.write(str(filename), audio[0],\n",
    "                     samplerate=spectrograms_helper.fs_hz, format='WAV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_bottom.local_class_conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_audio_player(audio_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pitch, sample in zip(unconditional_sampling_pitches, samples_batch):\n",
    "    filename = unconditional_generation_path / f'{instrument}-{pitch}.wav'\n",
    "    audio = spectrograms_helper.to_audio(sample.unsqueeze(0)).cpu().numpy()\n",
    "    if NORMALIZE_AUDIO:\n",
    "        sample /= abs(sample.max())\n",
    "    sf.write(str(filename), audio[0],\n",
    "             samplerate=spectrograms_helper.fs_hz, format='WAV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -C ./paper -cf ./paper/unconditional_generation.tar.gz unconditional_generation/20200309-220303-d006ab_436/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join([str(i) for i in unconditional_sampling_pitches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_bottom_sample_sequence = torch.LongTensor([127,504,30,501,419,175,292,283,43,43,153,159,189,203,266,510,427,427,490,488,183,509,263,401,442,473,393,358,219,15,435,325,103,210,102,478,326,485,319,52,167,121,250,10,327,228,155,167,373,195,30,401,80,220,342,211,283,494,427,478,492,252,429,257,366,148,494,429,511,361,472,83,61,494,467,157,385,420,427,78,78,69,326,326,385,327,121,220,429,157,69,326,275,126,453,385,157,492,77,54,385,157,77,121,120,327,78,398,126,134,134,492,80,396,134,492,134,396,385,134,327,385,472,61,80,460,54,246,446,74,490,306,12,444,153,86,196,439,283,490,359,478,30,502,501,509,483,366,294,60,387,270,330,325,393,260,364,266,196,501,319,127,359,450,127,215,510,9,168,107,208,266,102,427,431,378,129,36,203,409,131,54,309,54,154,110,221,395,279,89,49,220,1,210,503,259,175,494,49,157,145,77,103,155,427,77,221,472,167,413,29,138,83,78,59,157,412,121,494,429,494,220,61,355,157,413,251,350,121,412,398,54,49,220,78,121,472,54,398,78,54,54,121,78,78,49,157,49,157,398,78,281,326,188,367,396,53,12,415,30,159,444,437,164,153,102,294,325,43,219,358,110,153,415,308,173,450,260,166,338,330,298,46,258,319,127,364,478,292,455,24,153,349,313,108,264,238,298,334,319,323,445,237,86,161,108,83,86,389,49,121,435,454,355,61,326,123,412,309,123,221,350,155,453,253,494,83,83,361,398,175,78,175,78,412,198,49,385,77,54,121,157,398,54,78,413,385,413,69,54,361,78,54,454,412,253,69,472,157,121,138,54,121,121,121,54,78,121,429,54,78,78,413,78,361,77,54,115,412,367,412,80,237,249,509,108,162,472,432,359,260,183,490,325,59,175,450,502,103,43,159,478,106,497,125,56,421,156,330,52,248,258,292,159,364,355,485,325,33,131,485,67,309,457,448,211,59,131,33,146,13,78,78,183,326,111,193,189,148,385,342,309,492,195,58,89,385,429,54,119,89,453,472,253,89,80,350,494,80,120,54,121,429,396,420,362,89,318,295,220,420,126,327,420,385,472,385,220,80,89,138,492,492,351,134,140,318,89,420,472,388,276,416,492,126,117,276,472,134,134,472,385,492,89,121,59,429,385,423,157,140,126])\n",
    "# id_top_sample_sequence = torch.LongTensor([198,339,141,423,408,198,1,415,140,173,307,430,198,341,430,198,29,106,323,251,25,364,323,479,286,295,395,385,389,389,358,78,389,468,340,482,482,425,35,454,140,262,358,445,382,150,134,489,511,75,134,389,25,451,207,325,489,134,437,221,451,389,389,295,348,256,340,281,50,425,165,140,22,134,265,429,483,291,74,90,33,75,72,66,429,197,1,469,134,505,323,207,129,451,325,139,90,266,319,92,468,92,104,266,197,459,90,90,468,224,92,339,323,90,252,207,21,257,198,92,134,124,339,207,129,437,451,134])\n",
    "id_bottom_sample_sequence = torch.LongTensor([327,361,361,184,385,253,423,431,412,494,478,358,437,435,213,118,189,454,319,319,433,148,494,229,94,477,506,179,386,399,427,134,479,413,42,463,465,193,61,4,301,140,208,419,89,80,427,211,429,350,78,281,200,351,321,115,344,126,295,429,50,413,413,319,321,158,54,331,14,460,298,472,190,328,228,138,433,40,65,257,321,304,155,246,178,178,278,7,252,178,318,374,269,269,481,481,178,269,481,481,178,178,481,481,178,269,21,481,178,178,481,481,178,269,481,481,269,178,481,481,62,269,353,174,178,477,353,50,298,79,407,446,478,319,229,281,365,441,55,348,101,417,75,208,510,30,30,435,78,10,175,71,441,49,348,431,435,386,115,350,108,355,509,427,80,341,123,306,267,446,164,15,77,350,237,251,407,511,211,366,484,447,162,8,420,39,362,83,221,501,413,366,412,407,220,91,435,492,455,220,3,385,3,416,251,27,145,257,13,386,106,396,418,353,7,418,120,7,138,353,7,7,353,353,353,328,353,353,7,318,7,318,460,7,128,353,353,353,353,353,353,353,353,353,353,7,353,353,481,126,460,188,340,126,278,134,279,154,15,427,230,358,154,263,497,30,364,60,213,334,272,44,319,298,478,43,472,106,103,107,375,431,347,287,430,327,29,295,108,455,371,111,253,204,120,341,334,511,506,367,350,103,15,407,175,161,154,367,290,103,305,435,148,61,220,412,251,367,220,30,195,210,29,35,167,472,29,472,508,429,3,385,49,140,251,290,419,50,337,433,7,418,418,418,429,353,429,353,353,353,353,353,353,353,353,353,353,388,7,140,328,481,433,7,353,353,353,353,353,353,353,353,353,353,353,353,460,188,460,386,278,140,418,492,198,358,309,447,490,53,183,405,450,432,447,179,101,112,289,179,444,53,475,229,161,319,242,193,260,30,63,447,145,494,29,492,441,367,237,191,249,74,115,503,313,65,179,447,281,198,419,155,43,237,106,83,181,103,220,295,138,220,138,472,77,501,49,83,389,487,398,398,110,138,78,134,128,420,396,492,251,162,39,246,189,477,472,327,418,252,318,50,50,418,126,126,328,328,126,126,328,418,126,134,252,481,134,386,252,418,126,120,418,418,134,134,418,418,134,134,418,418,134,134,418,481,134,134,81,117,120,157])\n",
    "id_top_sample_sequence = torch.LongTensor([74,489,273,301,411,382,319,394,267,307,348,41,507,54,351,92,122,347,453,129,348,414,453,221,391,325,198,221,221,221,483,97,207,90,340,262,511,270,408,219,394,511,282,482,245,133,453,90,347,142,90,23,25,92,339,221,221,463,334,414,221,221,449,389,174,334,163,489,43,469,408,99,22,213,469,169,266,133,21,134,295,25,90,1,35,451,339,295,389,301,45,295,295,221,21,21,449,165,378,199,165,193,333,259,482,86,501,339,482,137,476,124,198,445,449,137,391,505,437,389,221,92,45,295,391,221,97,21])\n",
    "sample_name = \"organ_electronic_028-048-050.wav\"  # from the NSynth test split\n",
    "\n",
    "id_bottom_sample_codemap = transformer_bottom.to_time_frequency_map(\n",
    "    id_bottom_sample_sequence.unsqueeze(0),\n",
    "    kind='target')\n",
    "id_top_sample_codemap = transformer_top.to_time_frequency_map(\n",
    "    id_top_sample_sequence.unsqueeze(0),\n",
    "    kind='target')\n",
    "sample_original_pitch = 48\n",
    "\n",
    "def upsample_mask(mask_top):\n",
    "    mask_bottom = mask_top.clone()\n",
    "    \n",
    "    for repeat_dim in [1, 2]:\n",
    "        repeats = (transformer_bottom.shape[repeat_dim-1]\n",
    "                   // transformer_top.shape[repeat_dim-1])\n",
    "        mask_bottom = torch.repeat_interleave(\n",
    "            mask_bottom, repeats=repeats, dim=repeat_dim)\n",
    "    return mask_bottom\n",
    "\n",
    "two_seconds_all_frequencies_mask_top = torch.full_like(id_top_sample_codemap,\n",
    "                                                       fill_value=False, dtype=bool)\n",
    "two_seconds_all_frequencies_mask_top[:, :, :2] = True\n",
    "\n",
    "last_three_seconds_all_frequencies_mask_top = torch.full_like(id_top_sample_codemap,\n",
    "                                                              fill_value=False, dtype=bool)\n",
    "last_three_seconds_all_frequencies_mask_top[:, :, 1:] = True\n",
    "\n",
    "all_duration_bottom_half_frequencies_mask_top = torch.full_like(id_top_sample_codemap,\n",
    "                                                                fill_value=False, dtype=bool)\n",
    "all_duration_bottom_half_frequencies_mask_top[:, :transformer_top.shape[0]//2, :] = True\n",
    "\n",
    "all_duration_bottom_quarter_frequencies_mask_top = torch.full_like(id_top_sample_codemap,\n",
    "                                                                fill_value=False, dtype=bool)\n",
    "all_duration_bottom_quarter_frequencies_mask_top[:, :transformer_top.shape[0]//4, :] = True\n",
    "\n",
    "named_masks = {\n",
    "#     'two_seconds_all_frequencies': {\n",
    "#         'top': two_seconds_all_frequencies_mask_top,\n",
    "#         'bottom': upsample_mask(two_seconds_all_frequencies_mask_top)\n",
    "#     },\n",
    "#     'last_three_seconds_all_frequencies': {\n",
    "#         'top': last_three_seconds_all_frequencies_mask_top,\n",
    "#         'bottom': upsample_mask(last_three_seconds_all_frequencies_mask_top)\n",
    "#     },\n",
    "#     'all_duration_bottom_half_frequencies': {\n",
    "#         'top': all_duration_bottom_half_frequencies_mask_top,\n",
    "#         'bottom': upsample_mask(all_duration_bottom_half_frequencies_mask_top)\n",
    "#     },\n",
    "    'all_duration_bottom_quarter_frequencies': {\n",
    "        'top': all_duration_bottom_quarter_frequencies_mask_top,\n",
    "        'bottom': upsample_mask(all_duration_bottom_quarter_frequencies_mask_top)\n",
    "    },\n",
    "#     'keep_top_resample_full_bottom': {\n",
    "#         'top': torch.full_like(id_top_sample_codemap, fill_value=False, dtype=bool),\n",
    "#         'bottom': torch.full_like(id_bottom_sample_codemap, fill_value=True, dtype=bool),\n",
    "#     },\n",
    "#     'resample_full_top_keep_bottom': {\n",
    "#         'top': torch.full_like(id_top_sample_codemap, fill_value=True, dtype=bool),\n",
    "#         'bottom': torch.full_like(id_bottom_sample_codemap, fill_value=False, dtype=bool),\n",
    "#     }\n",
    "}\n",
    "\n",
    "inpainting_pitches = np.array([24, 36, 43, 48, 60, 64, 67, 72])\n",
    "inpainting_instruments = all_instrument_labels\n",
    "num_pitches_inpainting = len(inpainting_pitches)\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 0.8\n",
    "num_samples_per_class = 4\n",
    "class_conditioning_tensors_inpainting = {}\n",
    "\n",
    "encoded_inpainting_pitches = label_encoders_per_conditioning['pitch'].transform(\n",
    "    inpainting_pitches).transpose()\n",
    "class_conditioning_tensors_inpainting['pitch'] = (\n",
    "    torch.from_numpy(encoded_inpainting_pitches).long()\n",
    "    .repeat_interleave(num_samples_per_class)\n",
    "    .to(DEVICE))\n",
    "\n",
    "masked_inpainting_path = pathlib.Path(f'./paper/masked_inpainting/{vqvae_run_ID}_{checkpoint_epoch}/temp_{TEMPERATURE}-top_p_{TOP_P}')\n",
    "masked_inpainting_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for mask_name, masks in named_masks.items():\n",
    "    save_dir_path = masked_inpainting_path / mask_name\n",
    "    save_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Mask: {mask_name}\")\n",
    "    for instrument in inpainting_instruments:\n",
    "        print(f\"Instrument: {instrument}\")\n",
    "        encoded_instrument = label_encoders_per_conditioning['instrument_family_str'].transform(\n",
    "            [instrument]).transpose()\n",
    "        class_conditioning_tensors_inpainting['instrument_family_str'] = (\n",
    "            torch.from_numpy(encoded_instrument).long()\n",
    "            .repeat(num_pitches_inpainting)\n",
    "            .repeat_interleave(num_samples_per_class)\n",
    "            .to(DEVICE))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            transformer_top.to(DEVICE)\n",
    "            transformer_top.eval()\n",
    "            inpainted_top = sample_model(\n",
    "                transformer_top, device=DEVICE,\n",
    "                batch_size=num_pitches_inpainting*num_samples_per_class,\n",
    "                class_conditioning=class_conditioning_tensors_inpainting,\n",
    "                initial_code=id_top_sample_codemap.to(DEVICE).repeat((num_pitches_inpainting*num_samples_per_class, 1, 1)),\n",
    "                codemap_size=transformer_top.shape,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p_sampling_p=TOP_P,\n",
    "                use_multi_gpus=True,\n",
    "                mask=masks['top'].to(DEVICE),\n",
    "                progressbar_decorator=tqdm_notebook\n",
    "            )\n",
    "        \n",
    "            transformer_bottom.to(DEVICE)\n",
    "            transformer_bottom.eval()\n",
    "            inpainted_bottom = sample_model(\n",
    "                transformer_bottom,\n",
    "                device=DEVICE,\n",
    "                batch_size=num_pitches_inpainting*num_samples_per_class,\n",
    "                condition=inpainted_top,\n",
    "                initial_code=id_bottom_sample_codemap.to(DEVICE).repeat((num_pitches_inpainting*num_samples_per_class, 1, 1)),\n",
    "                class_conditioning=class_conditioning_tensors_inpainting,\n",
    "                codemap_size=transformer_bottom.shape,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p_sampling_p=TOP_P,\n",
    "                use_multi_gpus=True,\n",
    "                mask=masks['bottom'].to(DEVICE),\n",
    "                progressbar_decorator=tqdm_notebook\n",
    "            )\n",
    "\n",
    "            vqvae.to(DEVICE)\n",
    "            vqvae.eval()\n",
    "            samples_batch = vqvae.decode_code(inpainted_top, inpainted_bottom)\n",
    "            for pitch, sample in zip(inpainting_pitches, samples_batch.split(num_samples_per_class)):\n",
    "                filename = save_dir_path / f'{instrument}-{pitch}.wav'\n",
    "                audio = make_audio(sample)\n",
    "                if NORMALIZE_AUDIO:\n",
    "                    sample /= abs(sample.max())\n",
    "                sf.write(str(filename), audio,\n",
    "                         samplerate=spectrograms_helper.fs_hz, format='WAV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoded_sample = vqvae.decode_code(id_top_sample_codemap.to(DEVICE),\n",
    "                                       id_bottom_sample_codemap.to(DEVICE))\n",
    "    print(decoded_sample.shape)\n",
    "    make_audio_player(decoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -C paper/masked_inpainting/20200309-220303-d006ab_436/temp_1-top_p_0.8/ -cf ./paper/masked_inpainting.tar.gz all_duration_bottom_quarter_frequencies/ keep_top_resample_full_bottom/ two_seconds_all_frequencies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_code, bottom_code = vqvae.forward(spectrograms_helper.to_spectrogram(torch.zeros(1, 100000).to(DEVICE)))[-2:]\n",
    "print(top_code.shape)\n",
    "print(bottom_code.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae.resolution_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
