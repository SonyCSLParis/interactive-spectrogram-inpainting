{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vqvae import VQVAE\n",
    "from scheduler import CycleScheduler\n",
    "\n",
    "from nsynth_dataset import NSynthDataset\n",
    "\n",
    "import GANsynth_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "nsynth_dataset_path = pathlib.Path('~/code/data/nsynth/valid/hdf5').expanduser()\n",
    "nsynth_dataset = NSynthDataset(\n",
    "    root_path=nsynth_dataset_path,\n",
    "    use_mel_frequency_scale=True)\n",
    "num_workers = 0\n",
    "batch_size = 64\n",
    "loader = DataLoader(nsynth_dataset, batch_size=1,\n",
    "                    num_workers=num_workers, shuffle=True)\n",
    "in_channel = 2\n",
    "\n",
    "vqvae_decoder_activation = None\n",
    "\n",
    "dataloader_for_gansynth_normalization = None\n",
    "normalizer_statistics = None\n",
    "\n",
    "use_precomputed_normalization_statistics = True\n",
    "if use_precomputed_normalization_statistics:\n",
    "    normalizer_statistics_path = nsynth_dataset_path / '../normalization_statistics.pkl'\n",
    "    with open(normalizer_statistics_path, 'rb') as f:\n",
    "        normalizer_statistics = pickle.load(f)\n",
    "else:\n",
    "    dataloader_for_gansynth_normalization = loader\n",
    "\n",
    "print(\"Initializing model\")\n",
    "vqvae_path = pathlib.Path('../vq-vae-2-pytorch/checkpoint/vqvae_nsynth_560.pt')\n",
    "vqvae = VQVAE(in_channel=in_channel,\n",
    "              decoder_output_activation=vqvae_decoder_activation,\n",
    "              normalizer_statistics=normalizer_statistics,\n",
    "              dataloader_for_gansynth_normalization=dataloader_for_gansynth_normalization\n",
    "              )\n",
    "vqvae.load_state_dict(torch.load(vqvae_path))\n",
    "model = vqvae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae.eval()\n",
    "iterator = iter(loader)\n",
    "data_normalizer = vqvae.data_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_plus_phase(mag, IF):\n",
    "    mag =  np.exp(mag) - 1.0e-6\n",
    "    reconstruct_magnitude = np.abs(mag)\n",
    "\n",
    "    reconstruct_phase_angle = np.cumsum(IF * np.pi, axis=1)\n",
    "    stft = GANsynth_pytorch.phase_operation.polar2rect(reconstruct_magnitude, reconstruct_phase_angle)\n",
    "    inverse = librosa.istft(stft, hop_length = 512, win_length=2048, window = 'hann')\n",
    "\n",
    "    return inverse\n",
    "\n",
    "def convert_representation(sample):\n",
    "    channel_dimension = 0\n",
    "    spec = sample.select(channel_dimension, 0).data.cpu().numpy()\n",
    "    IF = sample.select(channel_dimension, 1).data.cpu().numpy()\n",
    "    back_mag, back_IF = GANsynth_pytorch.spectrograms_helper.melspecgrams_to_specgrams(spec, IF)\n",
    "    back_mag = np.vstack((back_mag,back_mag[1023]))\n",
    "    back_IF = np.vstack((back_IF,back_IF[1023]))\n",
    "    audio = mag_plus_phase(back_mag,back_IF)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iterator)[0][0][0]\n",
    "sample_audio = convert_representation(sample)\n",
    "print(\"Original audio\")\n",
    "IPython.display.display(IPython.display.Audio(sample_audio, rate=16000))\n",
    "\n",
    "reconstructed = vqvae.forward(sample.to(device))[0][0]\n",
    "reconstructed_audio = convert_representation(reconstructed)\n",
    "print(\"Reconstructed audio\")\n",
    "IPython.display.display(IPython.display.Audio(reconstructed_audio, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display computed codes\n",
    "codes_top = vqvae.quantize_t.embed.data.cpu().numpy()\n",
    "plt.matshow(codes_top)\n",
    "plt.title('Top layer (1st, unconditioned layer) codes')\n",
    "\n",
    "codes_bottom = vqvae.quantize_b.embed.data.cpu().numpy()\n",
    "plt.matshow(codes_bottom)\n",
    "plt.title('Bottom layer (2nd, conditioned layer) codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random codes for synthesis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
