{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vqvae import VQVAE, InferenceVQVAE\n",
    "from scheduler import CycleScheduler\n",
    "\n",
    "from nsynth_dataset import NSynthH5Dataset\n",
    "from GANsynth_pytorch.pytorch_nsynth_lib.nsynth import (\n",
    "    NSynth, get_mel_spectrogram_and_IF)\n",
    "\n",
    "import GANsynth_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "valid_pitch_range = [24, 84]\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "\n",
    "dataset_type = 'wav'\n",
    "nsynth_dataset_path = pathlib.Path('~/code/data/nsynth/valid/json_wav').expanduser()\n",
    "if dataset_type == 'hdf5':\n",
    "    nsynth_dataset = NSynthDataset(\n",
    "        root_path=nsynth_dataset_path,\n",
    "        use_mel_frequency_scale=True)\n",
    "elif dataset_type == 'wav':\n",
    "    def chained_transform(sample):\n",
    "                mel_spec, mel_IF = get_mel_spectrogram_and_IF(\n",
    "                    sample, hop_length=HOP_LENGTH)\n",
    "                mel_spec_and_IF_as_image_tensor = NSynthH5Dataset._to_image(\n",
    "                    [a.astype(np.float32)\n",
    "                     for a in [mel_spec, mel_IF]])\n",
    "                return mel_spec_and_IF_as_image_tensor\n",
    "    to_mel_spec_and_if = transforms.Lambda(chained_transform)\n",
    "    nsynth_dataset = NSynth(\n",
    "        root=str(nsynth_dataset_path),\n",
    "        transform=chained_transform,\n",
    "        valid_pitch_range=valid_pitch_range,\n",
    "        categorical_field_list=[],\n",
    "        convert_to_float=True)\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized dataset type\")\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 64\n",
    "loader = DataLoader(nsynth_dataset, batch_size=1,\n",
    "                    num_workers=num_workers, shuffle=True)\n",
    "in_channel = 2\n",
    "\n",
    "vqvae_decoder_activation = None\n",
    "\n",
    "dataloader_for_gansynth_normalization = None\n",
    "normalizer_statistics = None\n",
    "\n",
    "disable_input_normalization = True\n",
    "use_precomputed_normalization_statistics = True\n",
    "if not disable_input_normalization:\n",
    "    if use_precomputed_normalization_statistics:\n",
    "        normalizer_statistics_path = nsynth_dataset_path / '../normalization_statistics.pkl'\n",
    "        with open(normalizer_statistics_path, 'rb') as f:\n",
    "            normalizer_statistics = pickle.load(f)\n",
    "    else:\n",
    "        dataloader_for_gansynth_normalization = loader\n",
    "\n",
    "print(\"Initializing model\")\n",
    "vqvae_path = pathlib.Path('../vq-vae-2-pytorch/checkpoints/086f45/vqvae_nsynth_020.pt')\n",
    "vqvae = VQVAE(in_channel=in_channel,\n",
    "              decoder_output_activation=vqvae_decoder_activation,\n",
    "              normalizer_statistics=normalizer_statistics,\n",
    "              dataloader_for_gansynth_normalization=dataloader_for_gansynth_normalization,\n",
    "              groups=2\n",
    "              )\n",
    "vqvae.load_state_dict(torch.load(vqvae_path))\n",
    "vqvae.to(device)\n",
    "inference_vqvae = InferenceVQVAE(vqvae, device)\n",
    "# model = vqvae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae.eval()\n",
    "iterator = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, reconstructions = inference_vqvae.sample_reconstructions(loader)\n",
    "\n",
    "sample_audio = inference_vqvae.mag_and_IF_to_audio(samples,\n",
    "                                                   use_mel_frequency=True)\n",
    "print(\"Original audio\")\n",
    "IPython.display.display(IPython.display.Audio(sample_audio, rate=16000))\n",
    "\n",
    "reconstructions_audio = inference_vqvae.mag_and_IF_to_audio(reconstructions,\n",
    "                                                            use_mel_frequency=True)\n",
    "print(\"Reconstructed audio\")\n",
    "IPython.display.display(IPython.display.Audio(reconstructions_audio, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display computed codes\n",
    "codes_top = vqvae.quantize_t.embed.data.cpu()\n",
    "plt.matshow(codes_top - codes_top.mean(dim=0, keepdim=True))\n",
    "plt.title('Top layer (1st, unconditioned layer) codes', pad=20)\n",
    "plt.show()\n",
    "\n",
    "codes_bottom = vqvae.quantize_b.embed.data.cpu()\n",
    "plt.matshow(codes_bottom - codes_bottom.mean(dim=0, keepdim=True))\n",
    "plt.title('Bottom layer (2nd, conditioned layer) codes', pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code distribution over the training dataset\n",
    "num_embeddings_top = vqvae.quantize_t.n_embed\n",
    "num_embeddings_bottom = vqvae.quantize_b.n_embed\n",
    "code_usage_top = np.zeros(num_embeddings_top)\n",
    "code_usage_bottom = np.zeros(num_embeddings_bottom)\n",
    "\n",
    "def count_uses(codes_batch, code_usage_matrix):\n",
    "    for tensor in codes_batch:\n",
    "        for row in tensor:\n",
    "            for value in row:\n",
    "                code_usage_matrix[int(value)] += 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, pitch in tqdm(loader):\n",
    "        _, _, _, id_t, id_b = vqvae.encode(batch.to(device))\n",
    "        count_uses(id_t, code_usage_top)\n",
    "        count_uses(id_b, code_usage_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1, title=\"Bottom layer\")\n",
    "plt.bar(range(num_embeddings_bottom), height=list(np.log(code_usage_bottom + 1)))\n",
    "plt.subplot(1, 2, 2, title=\"Top layer\")\n",
    "plt.bar(range(num_embeddings_top), height=list(np.log(code_usage_top + 1)))\n",
    "\n",
    "plt.suptitle(\"Code usage (in log-scale)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
