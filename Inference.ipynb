{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vqvae import VQVAE, InferenceVQVAE\n",
    "from scheduler import CycleScheduler\n",
    "\n",
    "from nsynth_dataset import NSynthH5Dataset\n",
    "from GANsynth_pytorch.pytorch_nsynth_lib.nsynth import (\n",
    "    NSynth, make_to_mel_spec_and_IF_image_transform,\n",
    "    WavToSpectrogramDataLoader)\n",
    "\n",
    "import GANsynth_pytorch\n",
    "from GANsynth_pytorch.utils import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "valid_pitch_range = [24, 84]\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "FS_HZ = 16000\n",
    "USE_MEL_FREQUENCY = True\n",
    "\n",
    "\n",
    "dataset_type = 'wav'\n",
    "nsynth_dataset_path = pathlib.Path('~/code/data/nsynth/valid/json_wav').expanduser()\n",
    "if dataset_type == 'hdf5':\n",
    "    nsynth_dataset = NSynthDataset(\n",
    "        root_path=nsynth_dataset_path,\n",
    "        use_mel_frequency_scale=True)\n",
    "elif dataset_type == 'wav':\n",
    "    nsynth_dataset = NSynth(\n",
    "        root=str(nsynth_dataset_path),\n",
    "        valid_pitch_range=valid_pitch_range,\n",
    "        categorical_field_list=[],\n",
    "        convert_to_float=True)\n",
    "else:\n",
    "    raise ValueError(\"Unrecognized dataset type\")\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 64\n",
    "loader = WavToSpectrogramDataLoader(nsynth_dataset, batch_size=batch_size,\n",
    "                                    num_workers=num_workers, shuffle=True,\n",
    "                                    device=device)\n",
    "single_sample_loader = WavToSpectrogramDataLoader(nsynth_dataset, batch_size=1,\n",
    "                                                  num_workers=0, shuffle=True,\n",
    "                                                  device=device)\n",
    "\n",
    "in_channel = 2\n",
    "\n",
    "vqvae_decoder_activation = None\n",
    "\n",
    "dataloader_for_gansynth_normalization = None\n",
    "normalizer_statistics = None\n",
    "\n",
    "disable_input_normalization = True\n",
    "use_precomputed_normalization_statistics = True\n",
    "if not disable_input_normalization:\n",
    "    if use_precomputed_normalization_statistics:\n",
    "        normalizer_statistics_path = nsynth_dataset_path / '../normalization_statistics.pkl'\n",
    "        with open(normalizer_statistics_path, 'rb') as f:\n",
    "            normalizer_statistics = pickle.load(f)\n",
    "    else:\n",
    "        dataloader_for_gansynth_normalization = loader\n",
    "\n",
    "print(\"Initializing model\")\n",
    "run_ID = '20191022-155506-0839e0'\n",
    "checkpoint_epoch = '132'\n",
    "vqvae_path = pathlib.Path(f'../vq-vae-2-pytorch/checkpoints/{run_ID}/vqvae_nsynth_{checkpoint_epoch}.pt')\n",
    "vqvae_parameters_path = pathlib.Path(f'../vq-vae-2-pytorch/checkpoints/{run_ID}/model_parameters.json')\n",
    "with open(vqvae_parameters_path, 'r') as f:\n",
    "    vqvae_parameters = json.load(f)\n",
    "\n",
    "vqvae = VQVAE(decoder_output_activation=vqvae_decoder_activation,\n",
    "              dataloader_for_gansynth_normalization=dataloader_for_gansynth_normalization,\n",
    "              normalizer_statistics=normalizer_statistics,\n",
    "              **vqvae_parameters\n",
    "             )\n",
    "vqvae.load_state_dict(torch.load(vqvae_path, map_location=device))\n",
    "vqvae.to(device)\n",
    "vqvae.eval()\n",
    "    \n",
    "inference_vqvae = InferenceVQVAE(vqvae, device, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "\n",
    "def make_audio(mag_and_IF_batch: torch.Tensor) -> np.ndarray:\n",
    "    audio_batch = inference_vqvae.mag_and_IF_to_audio(mag_and_IF_batch,\n",
    "                                                      use_mel_frequency=USE_MEL_FREQUENCY)\n",
    "    audio_mono_concatenated = audio_batch.flatten().cpu().numpy()\n",
    "    return audio_mono_concatenated\n",
    "\n",
    "def make_audio_player(mag_and_IF_batch: torch.Tensor) -> None:\n",
    "    audio_mono_concatenated = make_audio(mag_and_IF_batch)\n",
    "    IPython.display.display(IPython.display.Audio(audio_mono_concatenated,\n",
    "                                                  rate=FS_HZ, normalize=True))\n",
    "    \n",
    "def plot_specs_and_IFs(*specs_and_IFs) -> None:    \n",
    "    num_subplots = 2*len(specs_and_IFs)\n",
    "    plots_per_row = 12\n",
    "    num_rows = num_subplots // plots_per_row + (\n",
    "        1 if num_subplots % plots_per_row != 0 or num_subplots < plots_per_row else 0)\n",
    "    plt.subplots(num_rows, plots_per_row, figsize=(25, 10*num_rows))\n",
    "    for subplot_index, spec_and_IF in enumerate(specs_and_IFs):\n",
    "        spec_and_IF = spec_and_IF.cpu().numpy()\n",
    "        ax_spec = plt.subplot(1, num_subplots, 1 + 2*subplot_index)\n",
    "        ax_IF = plt.subplot(1, num_subplots, 1+ 2*subplot_index + 1)\n",
    "        plots.plot_mel_representations(spec_and_IF[0], spec_and_IF[1],\n",
    "                                       hop_length=HOP_LENGTH, fs_hz=16000,\n",
    "                                       ax_spec=ax_spec, ax_IF=ax_IF)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, reconstructions = inference_vqvae.sample_reconstructions(single_sample_loader)\n",
    "\n",
    "# using InferenceVQVAE object\n",
    "sample_audio = inference_vqvae.mag_and_IF_to_audio(samples,\n",
    "                                                   use_mel_frequency=True)\n",
    "print(\"Original audio\")\n",
    "def print_stats(melspec_and_IF_batch: torch.Tensor) -> None:\n",
    "    logmelspec_batch = melspec_and_IF_batch[:,0]\n",
    "    melIF_batch = melspec_and_IF_batch[:,1]\n",
    "    print(f'Log-mel-spec: Mean {logmelspec_batch.mean().item():.2f}, variance {logmelspec_batch.var().item():.2f}')\n",
    "    print(f'Mel-IF: Mean {melIF_batch.mean().item():.2f}, variance {melIF_batch.var().item():.2f}')\n",
    "\n",
    "print_stats(samples)\n",
    "IPython.display.display(IPython.display.Audio(sample_audio.flatten().cpu(), rate=16000))\n",
    "\n",
    "print(\"Reconstructed audio\")\n",
    "print_stats(reconstructions)\n",
    "# using shortcut helper function in the rest of this notebook\n",
    "make_audio_player(reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction  without using all hidden layers\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples, _ = next(iter(single_sample_loader))\n",
    "    quant_t, quant_b, *_ = vqvae.encode(samples.to(device))\n",
    "    decoded_only_bottom = vqvae.decode(torch.zeros(quant_t.shape).to(device), quant_b).cpu()\n",
    "    decoded_only_top = vqvae.decode(quant_t, torch.zeros(quant_b.shape).to(device)).cpu()\n",
    "    decoded = vqvae.decode(quant_t, quant_b).cpu()\n",
    "\n",
    "specs_and_IFs = [samples[0], decoded_only_bottom[0], decoded_only_top[0], decoded[0]]\n",
    "plot_specs_and_IFs(specs_and_IFs)\n",
    "# plt.suptitle('Original, Reconstructed using only bottom / only top / top and bottom', pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display computed codes\n",
    "named_code_layers = {'top': vqvae.quantize_t, 'bottom': vqvae.quantize_b}\n",
    "num_layers = len(named_code_layers.values())\n",
    "fig, _ = plt.subplots(num_layers, 2, figsize=(20, 5*num_layers))\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "for layer_index, (layer_name, code_layer) in enumerate(named_code_layers.items()):\n",
    "    codes = code_layer.embed.data.cpu().T\n",
    "    ax = plt.subplot(num_layers, 2, 1 + 2*layer_index)\n",
    "    codes_matrix = codes - codes.mean(dim=0, keepdim=True)\n",
    "    plt.matshow(codes_matrix, fignum=0)\n",
    "    plt.title(f'{layer_name} layer codes (layer number {layer_index+1})', pad=20)\n",
    "    ax = plt.subplot(num_layers, 2, 1 + 2*layer_index + 1)\n",
    "#     codes_selfsimilarity = codes_matrix @ codes_matrix.T\n",
    "    codes_correlation = np.corrcoef(codes_matrix)\n",
    "    pairwise_correlations = pdist(codes_matrix + 1e-7, 'correlation')\n",
    "    \n",
    "#     print(pairwise_cosine_similarity)\n",
    "#     plt.matshow(1 - abs(codes_correlation), fignum=0)\n",
    "    plt.hist(pairwise_correlations, bins=30)\n",
    "#     plt.colorbar()\n",
    "    plt.title(f'{str(layer_name).capitalize()} layer codes absolute correlation \\n (layer number {layer_index+1})', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code distribution over the training dataset\n",
    "num_embeddings_top = vqvae.quantize_t.n_embed\n",
    "num_embeddings_bottom = vqvae.quantize_b.n_embed\n",
    "code_usage_top = torch.zeros(num_embeddings_top).long().to(device)\n",
    "code_usage_bottom = torch.zeros(num_embeddings_bottom).long().to(device)\n",
    "\n",
    "def count_uses(codes_batch, code_usage_matrix):\n",
    "    num_codes = len(code_usage_matrix)\n",
    "    code_usage_batch = codes_batch.flatten().bincount(minlength=num_codes)\n",
    "    code_usage_matrix += code_usage_batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, pitch in tqdm(loader):\n",
    "        _, _, _, id_t, id_b, _, _ = vqvae.encode(batch.to(device))\n",
    "        count_uses(id_t, code_usage_top)\n",
    "        count_uses(id_b, code_usage_bottom)\n",
    "\n",
    "plt.subplots(2, 1, figsize=(30, 15))\n",
    "plt.subplot(2, 1, 1, title=\"Bottom layer\")\n",
    "plt.bar(range(num_embeddings_bottom), height=list(code_usage_bottom.cpu().numpy()))\n",
    "plt.subplot(2, 1, 2, title=\"Top layer\")\n",
    "plt.bar(range(num_embeddings_top), height=list(code_usage_top.cpu().numpy()))\n",
    "plt.savefig(f'code_usage-epoch_{checkpoint_epoch}.svg')\n",
    "\n",
    "plt.suptitle(\"Code usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings dimension comparison\n",
    "\n",
    "In this section, we compare the dimension of the training/target data and of the embeddings,\n",
    "in order to compute and check if the compression ratio actually makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_single_sample_iterator = iter(siptngle_sample_loader)\n",
    "sample_as_batch, _ = next(one_shot_single_sample_iterator)\n",
    "sample_as_batch\n",
    "sample = sample_as_batch[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    *results_as_batches, _, _ = vqvae.encode(sample_as_batch.to(device))\n",
    "    quant_t, quant_b, diff_t_plus_b, id_t, id_b = [\n",
    "        tensor[0].cpu() for tensor in results_as_batches]\n",
    "\n",
    "print('Original shape: ', sample.shape)\n",
    "print('Quant bottom shape: ', quant_b.shape)\n",
    "print('Quant top shape: ', quant_t.shape)\n",
    "print('Code bottom shape: ', id_b.shape)\n",
    "print('Code top shape: ', id_t.shape)\n",
    "print(\"Compression ratio: \", (id_b.numel() + id_t.numel()) / sample.numel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound creation\n",
    "\n",
    "In this section, we attempt to create new sounds by combining codes for heterogeneous sounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sample_loader = WavToSpectrogramDataLoader(nsynth_dataset, batch_size=2,\n",
    "                                               num_workers=2, shuffle=True)\n",
    "one_shot_two_sample_iterator = iter(two_sample_loader)\n",
    "with torch.no_grad():\n",
    "    samples, _ = next(one_shot_two_sample_iterator)\n",
    "    quant_t, quant_b, _, id_t, id_b, _, _ = vqvae.encode(samples.to(device))\n",
    "\n",
    "print(\"Original audios\")\n",
    "make_audio_player(samples)\n",
    "\n",
    "print('Reconstructions')\n",
    "with torch.no_grad():\n",
    "    reconstructions = vqvae.decode_code(id_t, id_b)\n",
    "make_audio_player(reconstructions)\n",
    "\n",
    "print('Exchange top and bottom codes')\n",
    "with torch.no_grad():\n",
    "    reconstructions_switched_codes = vqvae.decode_code(id_t, id_b.flip(0))\n",
    "make_audio_player(reconstructions_switched_codes)\n",
    "\n",
    "plot_specs_and_IFs(*samples[0:2], *reconstructions[0:2], *reconstructions_switched_codes[0:2])\n",
    "\n",
    "# print('Exchange half of the codes (in the temporal dimension)')\n",
    "# def switch_temporal_halves(codes_tensor: torch.Tensor) -> torch.Tensor:\n",
    "#     batch_dim, frequency_dim, temporal_dim = 0, 1, 2\n",
    "#     duration_n = codes_tensor.shape[temporal_dim]\n",
    "#     first_half = codes_tensor[:, :, :duration_n//2]\n",
    "#     second_half = codes_tensor[:, :, duration_n//2:]\n",
    "#     return torch.cat([first_half.flip(0), second_half], temporal_dim)\n",
    "# with torch.no_grad():\n",
    "#     reconstructions_halves_switched = vqvae.decode_code(\n",
    "#         *[switch_temporal_halves(codes) for codes in [id_t, id_b]])\n",
    "# make_audio_player(reconstructions_halves_switched)\n",
    "\n",
    "# print('Set half (temporally) of the latent map to zero')\n",
    "# def zero_temporal_halves(codes_tensor: torch.Tensor) -> torch.Tensor:\n",
    "#     batch_dim, frequency_dim, temporal_dim = 0, 1, 2\n",
    "#     duration_n = codes_tensor.shape[temporal_dim]\n",
    "#     first_half = codes_tensor[:, :, :duration_n//2]\n",
    "#     zero_second_half = torch.zeros(codes_tensor[:, :, duration_n//2:].shape, dtype=first_half.dtype).to(device)\n",
    "#     return torch.cat([first_half, zero_second_half], dim=temporal_dim)\n",
    "# with torch.no_grad():\n",
    "#     reconstructions_halves_zeroed = vqvae.decode_code(\n",
    "#         *[zero_temporal_halves(codes) for codes in [id_t, id_b]])\n",
    "# make_audio_player(reconstructions_halves_zeroed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear interpolation of the embedddings')\n",
    "def make_interpolations(start_map, end_map, steps):\n",
    "    ratios = torch.linspace(0, 1, steps).to(device)\n",
    "    translations = torch.einsum('i, jkl -> ijkl', ratios, (end_map - start_map))\n",
    "    return start_map + translations\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_steps = 5\n",
    "    interpolations_unquant_t_and_b = [\n",
    "        make_interpolations(codes[0], codes[1], num_steps)\n",
    "        for codes in (quant_t, quant_b)]\n",
    "    print(interpolations_unquant_t_and_b[0][:,0,0])\n",
    "    print(interpolations_unquant_t_and_b[0].shape)\n",
    "    print(interpolations_unquant_t_and_b[1].shape)\n",
    "    \n",
    "    interpolations_quant_t, interpolations_quant_b = [\n",
    "        quantizer(interpolations_unquant.permute(0, 2, 3, 1))[0].permute(0, 3, 1, 2)\n",
    "        for quantizer, interpolations_unquant in zip(\n",
    "            [vqvae.quantize_t, vqvae.quantize_b],\n",
    "            interpolations_unquant_t_and_b)\n",
    "    ]\n",
    "    \n",
    "    print(interpolations_quant_t[:,0,0])\n",
    "    interpolations = vqvae.decode(interpolations_quant_t, interpolations_quant_b)\n",
    "for index, interpolation in enumerate(interpolations):\n",
    "    print(f'Step {index}')\n",
    "    make_audio_player(interpolation)\n",
    "\n",
    "# comparing with the sum of the two signals in temporal domain\n",
    "make_audio_player(reconstructions.sum(0)/2)\n",
    "specs_and_IFs = [samples[0], samples[1], interpolations[1], interpolations[2], interpolations[3], samples.sum(0)]\n",
    "plot_specs_and_IFs(*specs_and_IFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: don't feed latent maps to quantizer by reshaping them, use .permute()\n",
    "# Otherwise, the results are scrambled!\n",
    "with torch.no_grad():\n",
    "    embeddings_t = quant_t[0].reshape(128, 16, 64)\n",
    "    requantized_embeddings_t = vqvae.quantize_t(embeddings_t)[0]\n",
    "    display(requantized_embeddings_t - embeddings_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent-codes distortion\n",
    "\n",
    "Here, we slightly modify the latent maps to check the effect on the decoded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, reconstruction = inference_vqvae.sample_reconstructions(single_sample_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    *results_as_batches, _, _ = vqvae.encode(sample.to(device))\n",
    "    quant_t, quant_b, diff_t_plus_b, id_t, id_b = [\n",
    "        tensor.cpu() for tensor in results_as_batches]\n",
    "\n",
    "all_code_layers = {'top': id_t, 'bottom': id_b}\n",
    "code_layers_to_corrupt_names = ['bottom']\n",
    "code_layers_to_keep_names = list(set(all_code_layers.keys()) - set(code_layers_to_corrupt_names))\n",
    "num_embeddings = {'top': vqvae.quantize_t.n_embed,\n",
    "                  'bottom': vqvae.quantize_b.n_embed}\n",
    "\n",
    "add_values = [+1, -1, +10, -10, +20, -20, +100, -100, +200, -200]\n",
    "id_plus_corrupted = {\n",
    "    code_layer_name: torch.cat([all_code_layers[code_layer_name]\n",
    "                                .add(add_value)\n",
    "                                .remainder(num_embeddings[code_layer_name])\n",
    "                                .long()\n",
    "     for add_value in add_values\n",
    "    ], 0)\n",
    "    for code_layer_name in code_layers_to_corrupt_names\n",
    "}\n",
    "\n",
    "id_plus_original = {\n",
    "    code_layer_name: all_code_layers[code_layer_name].repeat(len(add_values), 1, 1)\n",
    "    for code_layer_name in code_layers_to_keep_names\n",
    "}\n",
    "\n",
    "def pick_codes(original, corrupted):\n",
    "    codes = [(original.get(code_layer_name, None) if original.get(code_layer_name, None) is not None\n",
    "             else corrupted[code_layer_name]).to(device)\n",
    "            for code_layer_name in all_code_layers.keys()]\n",
    "    return codes\n",
    "\n",
    "# weights = torch.Tensor([0.2, 0.6, 0.2])\n",
    "# id_stochastic_1 = [\n",
    "#     id_map.add(torch.multinomial(weights, id_map.numel(), replacement=True).reshape(id_map.shape).add(-1).to(device)).remainder(n_embed).long()\n",
    "#     for id_map, n_embed in zip([id_t, id_b],\n",
    "#                                [vqvae.quantize_t.n_embed, vqvae.quantize_b.n_embed])\n",
    "# ]\n",
    "\n",
    "# id_stochastic_randn_0_20_corrupted = [\n",
    "#     id_map.add((torch.randn_like(id_map, dtype=float).to(device) * 20).long()).remainder(n_embed).long()\n",
    "#     for id_map, n_embed in zip(codes_to_corrupt,\n",
    "#                                num_embeddings_to_corrupt)\n",
    "# ]\n",
    "\n",
    "# print(id_stochastic_randn_0_20_corrupted)\n",
    "# print(codes_to_keep)\n",
    "# id_stochastic = id_stochastic_randn_0_20_corrupted + codes_to_keep\n",
    "# print(id_stochastic)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructions_plus = vqvae.decode_code(*pick_codes(id_plus_corrupted, id_plus_original))\n",
    "#     reconstructions_stochastic_1 = vqvae.decode_code(*id_stochastic_1)\n",
    "#     reconstructions_stochastic_randn_0_20 = vqvae.decode_code(*id_stochastic)\n",
    "\n",
    "print('Original')\n",
    "make_audio_player(sample)\n",
    "\n",
    "print('Reconstructions')\n",
    "make_audio_player(reconstruction)\n",
    "    \n",
    "for index, add_value in enumerate(add_values):\n",
    "    print('Uniform add', add_value)\n",
    "    make_audio_player(reconstructions_plus[index])\n",
    "\n",
    "# make_audio_player(reconstructions_stochastic_1)\n",
    "\n",
    "# make_audio_player(reconstructions_stochastic_randn_0_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.Tensor([0.2, 0.6, 0.2])\n",
    "id_stochastic = [\n",
    "    torch.multinomial(weights, id_map.numel(), replacement=True).reshape(id_map.shape).to(device).long()\n",
    "    for id_map, n_embed in zip([id_t, id_b],\n",
    "                               [vqvae.quantize_t.n_embed, vqvae.quantize_b.n_embed])\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructions_stochastic = vqvae.decode_code(*id_stochastic)\n",
    "make_audio_player(reconstructions_stochastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-domain application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
